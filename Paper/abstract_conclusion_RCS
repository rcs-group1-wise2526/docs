\begin{abstract}
Large language models (LLMs) are increasingly used as proxies for human respondents in psychological assessments, but their psychometric stability across languages and models remains underexplored. This study evaluates construct validity of the Big Five Inventory-2 (BFI-2), a 60-item personality measure with balanced true- and reverse-keyed items, administered to diverse open-weight models (Gemma, Llama, Qwen, Ministral) and closed models (GPT-5.1, DeepSeek-V3.2, Mistral) in English, Turkish, Chinese, and Spanish using Multi-Turn Decision Tracing (MTDT), which models responses as branching decision networks via constrained decoding, alongside traditional metrics like psychometric fingerprints, Exploratory Graph Analysis, Wasserstein distances, and Cronbach's alpha. Findings reveal pervasive response invariance, failures on reverse-keyed items, and model family as the dominant factor over language or size, with larger Ministral variants showing modest cross-language consistency (0.9+ alignment) yet superficial trait mimicry rather than structural fidelity, urging caution for LLMs in AI psychometrics and multilingual evaluations.
\end{abstract}

\textbf{Keywords} AI Psychometrics, Large Language Models, Big Five Inventory-2, Multi-Turn Decision Tracing, Measurement Invariance, Multilingual Evaluation

\section{Conclusion}
In conclusion, our comprehensive evaluation across multiple LLM families and languages reveals that while these models can superficially mimic human personality responses on the Big Five Inventory-2 (BFI-2), they consistently fall short of achieving true psychometric validity. Key limitations include pervasive response invariance (where models rigidly select uniform answer paths regardless of item content), failures to properly reverse-score items (leading to internal inconsistencies), and a dominant influence of model family origin over language or size effects, as evidenced by high cross-language alignment scores (0.9+) primarily within families like larger Ministral variants. These patterns suggest that LLM outputs reflect training data biases and linguistic artifacts more than stable latent traits, which challenges their reliability as proxies in psychological and social science research.

To advance AI psychometrics, we advocate standardized multilingual benchmarks that incorporate novel techniques like Multi-Turn Decision Tracing (MTDT), alongside traditional metrics such as psychometric fingerprints and Wasserstein distances, to better diagnose and mitigate these shortcomings. Future work should explore fine-tuning strategies or hybrid human-AI validation frameworks to enhance cross-lingual invariance, ensuring LLMs can be deployed responsibly in diverse global contexts without propagating invalid inferences. Ultimately, this study underscores the need for caution: LLMs should not yet substitute for human respondents in high-stakes assessments, but rather serve as diagnostic tools to illuminate the boundaries of machine psychology.
