\begin{abstract}
 Large language models (LLMs) present a conceivably invaluable asset to the field of the social sciences and psychological study. The ability to reduce costs
  by simulating human response data could allow for greatly expanded scopes of study, not to mention creating reliable new methodologies for researchers to quickly
  and easily gather survey data. This being said, however, their ability to truly simulate human psychological constructs (as opposed to a mere emulation of surface-level qualities of human answering)
  remains questionable \cite{munkerFingerprintingLLMsSurvey2025a}. This study aims to assess a diverse selection of open and closed source models (including but not limited to Gemma 3, Qwen3, GPT-5.1, DeepSeek-V3.2, etc.) using two different 
  model simulation methods and across multiple language domains on the Big Five Inventory-2 (BFI-2), a 60-item personality measure with balanced true- and reverse-keyed items, to explore just how different models
  operationalize commonly assessed psychological constructs under a variety of different circumstances. We hoped that through the use of our different simulation methods, Multi-Turn Decision Tracing (MTDT) which models
  responses as branching decisions, and through traditional model prompting techniques, we could evaluate model behavior, as it compares against human baselines, as it varies across languages, and as it correlates between models.
  To these ends, we observed pervasive response invariance, a problem previously described in literature on the topic [18], as well as an overall inability to maintain a reliable, internally consistent, grasp on psychological constructs. 
  Where we expected language might play an outsized role in affecting model handling of psychological modeling, the dominant factor came to be model origin/family, even over other important attributes such as model size. Overall, we are left
  with an impression of LLM capabilities that urges us to implore caution when exploring their potential use as human simulacra in the social sciences, lest one mistake superficial trait mimicry for psychometric structural fidelity. 

\textbf{Keywords} AI Psychometrics, Large Language Models, Big Five Inventory-2, Multi-Turn Decision Tracing, Measurement Invariance, Multilingual Evaluation

\section{Conclusion}
In conclusion, our comprehensive evaluation across multiple LLM families and languages reveals that while these models can superficially mimic human personality responses on the Big Five Inventory-2 (BFI-2), they consistently fall short of achieving true psychometric validity. Key limitations include pervasive response invariance (where models rigidly select uniform answer paths regardless of item content), failures to properly reverse-score items (leading to internal inconsistencies), and a dominant influence of model family origin over language or size effects, as evidenced by high cross-language alignment scores (0.9+) primarily within families like larger Ministral variants. These patterns suggest that LLM outputs reflect training data biases and linguistic artifacts more than stable latent traits, which challenges their reliability as proxies in psychological and social science research.

To advance AI psychometrics, we advocate standardized multilingual benchmarks that incorporate novel techniques like Multi-Turn Decision Tracing (MTDT), alongside traditional metrics such as psychometric fingerprints and Wasserstein distances, to better diagnose and mitigate these shortcomings. Future work should explore fine-tuning strategies or hybrid human-AI validation frameworks to enhance cross-lingual invariance, ensuring LLMs can be deployed responsibly in diverse global contexts without propagating invalid inferences. Ultimately, this study underscores the need for caution: LLMs should not yet substitute for human respondents in high-stakes assessments, but rather serve as diagnostic tools to illuminate the boundaries of machine psychology.
