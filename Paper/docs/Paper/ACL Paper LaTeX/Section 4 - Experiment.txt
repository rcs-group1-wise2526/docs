\section{Experiments}

\subsection{MTDT Method}

From our MTDT method testing across four model families, ten total model variations (at various sizes), and across three languages, we acquired over 430 pairwise similarity comparisons between the models to assess the cross-language robustness of the varying model’s psychometric patterns. Further, we produced a sankey diagram for each model to provide visual clarity to its decision- making path and quickly observe overarching patterns. To these ends, our primary observation was the lack of a significant impact from language choice or model size on model similarity scoring and or sankey behavior. Indeed, regardless of language, the primary factor that appeared to most closely dictate model behavior outcome was model family, though this was also to a relative degree. Qwen models revealed a fairly high similarity scoring with one another across all languages and models sizes, as did the Ministral models, somewhat.

However, the results from the Qwen models’ similarity scoring take on a new character when viewed within the context of their sankey diagrams. In this domain, almost all the Qwen models produced what we deem to be invariant results, that is to say, the models overwhelmingly selected the same answer path across every question, leading to very unchanging results.

Regardless of the answer invariance concern with most of the Qwen models (and some of the Llama models), a larger problem also stands out amidst even many of the ostensibly valid results wherein model output simply does not effectively operationalize BFI-2 psychometric constructs. In order to show alignment with BFI-2 constructs such as "Extraversion," or "Agreeableness," it is not enough that a model consistently (but not invaryingly) answers with values between 3 - 5 (indicating a range of neutral to strong agreement) because of the BFI-2’s various reverse-coded statements. High positive alignment with an attribute such as "Extraversion," is not simply a matter of indicating strong agreement with statements like "I am someone who is outgoing, sociable," but also one of indicating strong disagreement with reverse metric statements like "I am someone who rarely feels excited or eager." What we find instead is that many models selected paths that demonstrated alignment with metrics in traditional questions, but did not produce paths that accounted for reverse statements, producing an overall inconsistency that undermines the validity of their modeling of actual psychometric constructs.

Of the models that remain, the larger Ministral models (8B and 14B) showed the most consistency overall at demonstrating robust psychometric constructs, with some minor differences between languages, namely a more consistent performance in English and Chinese than Turkish.

\subsection{Traditional Method}

Using our traditional LLM querying pipeline, we prompted across three models and four languages over a hundred sample surveys and acquired a plentitude of various alignment parameters comparing model performance across BFI-2 psychometric domains and between models and baseline human BFI-2 data acquired for each language. Our results, however, also revealed a familiar problem that presented us with the difficult reality of being unable to utilize all evaluation metrics we had anticipated. Like with some of the models tested using the new (MTDT) method, we collected data that was by and large, highly invariant. Accordingly, we were unable to produce effective model fingerprints or EGA diagrams (except for Mistral), though we were able to assess the behavioral alignment of our tested models, both between model outputs in different languages and against human baselines. We further collected cronbach’s alpha data as a measure of internal consistency for GPT and Mistral.

\subsubsection{LLMs and Human Alignment}

Though we were not able to recreate the same fingerprinting technique used in [18] due to reasons of data invariancy, we were able to still observe a similar conclusion based on our analysis of model alignment against our human benchmark data across our selection of languages. Therein, no model gave a particularly strong alignment with any specific BFI-2 domain (Agreeableness, Conscientiousness, etc.) across all languages, nor did any model show especially strong alignment with any of the human answer sets in any language, though some were closer than others. These observations appear consistent with [18] on the basis that while these models simulate a human-like set of responses, their simulation cannot be said to be a completely accurate stand-in for true human-generated data.

\subsubsection{Alignment Across Models}

Model alignment results between models (i.e. the alignment of results between GPT and Mistral) remained remarkably consistent across all four languages. In every case, the models with the strongest alignment were GPT 5.1 and DeepSeek V3.2, with little to remark on in the way of relationships between the other models.

\subsubsection{Alignment Across Languages}

The most dynamic results we observed came in the form of our assessment of model alignment within the same model and across different languages. In this space, we found an overall great deal of item alignment across all BFI-2 domains, but a few key patterns came to the forefront. Namely, every model appeared to have a different grouping of language pairs that were more aligned with one another than others. In the case of GPT, generally, the most aligned pairings were English and Turkish and Spanish and Turkish. For DeepSeek, English and Chinese were a particularly divergent pairing while the others were fairly aligned. Finally, for Mistral, English and Chinese were by far the most aligned of any language pairing in the overall analysis, followed by a fairly aligned Spanish and Turkish result.

\subsubsection{Cronbach’s Alpha}

From our cronbach’s alpha analysis, GPT demonstrates fairly strong internal consistency metrics across all languages, with some degree of fluctuation in Spanish (particularly in the Negative Emotionality domain) and in Turkish generally. Mistral, by contrast, exhibits a highly irregular set of cronbach’s alpha metrics across all languages, with its strongest values representing domains in Spanish. Unfortunately, due to the highly invariant nature of the results from DeepSeek in particular, we were unable to conduct cronbach’s alpha testing on our data from DeepSeek.