\begin{document}

\begin{abstract}
Large language models (LLMs) are increasingly used as simulated survey respondents and behavioral proxies in computational social science, enabling scalable experimentation without human participants. Yet their validity depends on whether model outputs reflect coherent psychometric structure rather than superficial response patterns. We evaluate the psychometric validity of LLMs using the Big Five Inventory-2 (BFI-2), a 60-item personality instrument with balanced true- and reverse-keyed items, across English, Turkish, Chinese, and Spanish. Our framework combines Multi-Turn Decision Tracing (MTDT), which models survey completion as a sequential decision process, with a traditional prompting pipeline incorporating structural similarity and behavioral alignment metrics. Across model families and languages, we observe pervasive response invariance and frequent reverse-key incoherence, indicating systematic failures to operationalize latent personality constructs. While some models exhibit partial structural robustness, model family and origin exert a stronger influence on psychometric behavior than prompted language or model size. These findings challenge the assumption that LLMs can reliably substitute for human respondents in high-stakes social science settings and highlight the need for rigorous construct validation when deploying LLMs as behavioral proxies.
\end{abstract}