\section{Conclusion}

This work examined whether large language models preserve the psychometric structure of established personality instruments when used as simulated respondents. Addressing RQ1, we find pervasive response invariance and frequent reverse-key incoherence, indicating that many models fail to consistently operationalize opposing items within hierarchical constructs. Regarding RQ2, cross-lingual evaluation reveals uneven structural stability: psychometric patterns are not reliably preserved across English, Turkish, Chinese, and Spanish. Finally, in response to RQ3, model family and training origin exert a stronger influence on psychometric behavior than prompted language or model size.

Across both Multi-Turn Decision Tracing and traditional evaluation pipelines, LLM outputs often appear superficially plausible yet structurally misaligned with human latent trait organization. These results challenge the assumption that LLMs can serve as interchangeable proxies for human respondents in social science research. Instead, current models reflect architectural and training-data regularities more than stable psychological constructs. We therefore argue that LLMs should be deployed cautiously in behavioral simulation and treated as instruments for diagnosing the boundaries of machine psychology rather than as substitutes for human participants.

We note that future work in this field may wish to explore additional language domains and or implement targeted considerations to attempt to retrieve psychometrically valid outputs without the persistent invariance issue our team faced. Additionally, as the BFI-2 encompasses a relatively diverse set of human psychological constructs, future efforts to refine LLM abilities to effectively model these constructs may find more success honing in on a more specific set of constructs, such as those that found a greater alignment between human and LLM results in this experiment.